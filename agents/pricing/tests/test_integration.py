"""
LLM-Lite Parametric Pricing í†µí•© í…ŒìŠ¤íŠ¸

ì´ íŒŒì¼ì€ ì „ì²´ ì‹œìŠ¤í…œì˜ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ì‹¤ì œ LLM API í˜¸ì¶œ ì—†ì´ë„ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import asyncio
import json
from typing import Dict, Any

# í…ŒìŠ¤íŠ¸ìš© Mock ë°ì´í„°
MOCK_PERIL_CANVAS = {
    "peril": "server_downtime",
    "description": "ê²Œì„ ì„œë²„ ë‹¤ìš´íƒ€ì„ìœ¼ë¡œ ì¸í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì†ì‹¤",
    "trigger_metric": "downtime_minutes",
    "data_sources": ["Monitoring APIs", "Internal Systems"],
    "region": "Global",
    "coverage_period": "annual",
    "limit_structure": {
        "trigger_condition": {
            "metric": "downtime_minutes",
            "threshold": 10.0,
            "operator": ">=",
            "unit": "minutes"
        },
        "payout_curve": {
            "curve_type": "linear",
            "base_amount": 500.0,
            "max_payout": 750000.0,
            "multiplier": 500.0
        },
        "deductible": 0.0,
        "waiting_period": 0,
        "policy_period": 365
    }
}

MOCK_FREQUENCY_PRIOR = {
    "distribution": "negative_binomial",
    "parameters": {"r": 8.0, "p": 0.6},
    "percentiles": {"5th": 2.0, "50th": 5.0, "95th": 12.0},
    "sources": ["Industry Uptime Statistics", "Cloud Provider SLAs"],
    "confidence": 0.8
}

MOCK_SEVERITY_PRIOR = {
    "distribution": "exponential",
    "parameters": {"lambda": 0.1},
    "percentiles": {"5th": 0.5, "50th": 6.9, "95th": 30.0},
    "metric_unit": "minutes",
    "sources": ["IT Industry Reports"],
    "confidence": 0.75
}


class TestPerilCanvasGeneration:
    """Peril Canvas ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    def test_canvas_structure_validation(self):
        """Canvas êµ¬ì¡° ê²€ì¦"""
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = [
            "peril", "description", "trigger_metric", 
            "data_sources", "limit_structure"
        ]
        
        for field in required_fields:
            assert field in MOCK_PERIL_CANVAS, f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}"
        
        # Limit Structure ê²€ì¦
        limit_structure = MOCK_PERIL_CANVAS["limit_structure"]
        assert "trigger_condition" in limit_structure
        assert "payout_curve" in limit_structure
        
        # Trigger Condition ê²€ì¦
        trigger = limit_structure["trigger_condition"]
        assert trigger["threshold"] > 0
        assert trigger["operator"] in [">=", "<=", ">", "<", "=="]
        
        # Payout Curve ê²€ì¦
        payout = limit_structure["payout_curve"]
        assert payout["max_payout"] >= payout["base_amount"]
        assert payout["curve_type"] in ["linear", "step", "exponential", "logarithmic"]
    
    def test_event_type_mapping(self):
        """ì´ë²¤íŠ¸ íƒ€ì… ë§¤í•‘ ê²€ì¦"""
        
        event_mappings = {
            "ê²Œì„ ì„œë²„ ë‹¤ìš´íƒ€ì„": "server_downtime",
            "íƒœí’ ë³´í—˜": "typhoon",
            "í•­ê³µí¸ ì§€ì—°": "flight_delay",
            "ì§€ì§„ ìœ„í—˜": "earthquake"
        }
        
        for korean_input, expected_peril in event_mappings.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë¡œì§ ê²€ì¦
            normalized_input = korean_input.lower()
            
            if "ì„œë²„" in normalized_input or "ë‹¤ìš´" in normalized_input:
                assert expected_peril == "server_downtime"
            elif "íƒœí’" in normalized_input:
                assert expected_peril == "typhoon"
            elif "í•­ê³µ" in normalized_input or "ì§€ì—°" in normalized_input:
                assert expected_peril == "flight_delay"
            elif "ì§€ì§„" in normalized_input:
                assert expected_peril == "earthquake"


class TestPriorExtraction:
    """Prior ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    
    def test_frequency_prior_structure(self):
        """ë¹ˆë„ Prior êµ¬ì¡° ê²€ì¦"""
        
        freq_prior = MOCK_FREQUENCY_PRIOR
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ["distribution", "parameters", "percentiles", "confidence"]
        for field in required_fields:
            assert field in freq_prior, f"ë¹ˆë„ Prior í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}"
        
        # ë¶„í¬ íŒŒë¼ë¯¸í„° ê²€ì¦
        if freq_prior["distribution"] == "negative_binomial":
            params = freq_prior["parameters"]
            assert "r" in params and "p" in params
            assert params["r"] > 0 and 0 < params["p"] <= 1
        
        # ë°±ë¶„ìœ„ìˆ˜ ê²€ì¦
        percentiles = freq_prior["percentiles"]
        assert percentiles["5th"] <= percentiles["50th"] <= percentiles["95th"]
    
    def test_severity_prior_structure(self):
        """ì‹¬ë„ Prior êµ¬ì¡° ê²€ì¦"""
        
        sev_prior = MOCK_SEVERITY_PRIOR
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ["distribution", "parameters", "percentiles", "metric_unit"]
        for field in required_fields:
            assert field in sev_prior, f"ì‹¬ë„ Prior í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}"
        
        # ë¶„í¬ íŒŒë¼ë¯¸í„° ê²€ì¦
        if sev_prior["distribution"] == "exponential":
            params = sev_prior["parameters"]
            assert "lambda" in params
            assert params["lambda"] > 0
        
        # ë°±ë¶„ìœ„ìˆ˜ ê²€ì¦
        percentiles = sev_prior["percentiles"]
        assert percentiles["5th"] <= percentiles["50th"] <= percentiles["95th"]


class TestScenarioGeneration:
    """ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    def test_scenario_data_structure(self):
        """ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° êµ¬ì¡° ê²€ì¦"""
        
        # Mock ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
        mock_scenario = {
            "year": 1,
            "event_count": 2,
            "events_with_payouts": [
                {
                    "event_id": "1_0",
                    "severity": 15.5,
                    "payout": 7750.0,
                    "triggered": True
                },
                {
                    "event_id": "1_1", 
                    "severity": 5.2,
                    "payout": 0.0,
                    "triggered": False
                }
            ],
            "annual_loss": 7750.0
        }
        
        # êµ¬ì¡° ê²€ì¦
        assert "year" in mock_scenario
        assert "event_count" in mock_scenario
        assert "annual_loss" in mock_scenario
        assert mock_scenario["event_count"] == len(mock_scenario["events_with_payouts"])
        
        # ì´ë²¤íŠ¸ êµ¬ì¡° ê²€ì¦
        for event in mock_scenario["events_with_payouts"]:
            assert "severity" in event
            assert "payout" in event
            assert "triggered" in event
            assert event["payout"] >= 0
    
    def test_payout_calculation_logic(self):
        """ì§€ê¸‰ì•¡ ê³„ì‚° ë¡œì§ ê²€ì¦"""
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {"severity": 15.0, "threshold": 10.0, "operator": ">=", "expected_triggered": True},
            {"severity": 5.0, "threshold": 10.0, "operator": ">=", "expected_triggered": False},
            {"severity": 950.0, "threshold": 950.0, "operator": "<=", "expected_triggered": True},
            {"severity": 960.0, "threshold": 950.0, "operator": "<=", "expected_triggered": False}
        ]
        
        for case in test_cases:
            severity = case["severity"]
            threshold = case["threshold"]
            operator = case["operator"]
            expected = case["expected_triggered"]
            
            # íŠ¸ë¦¬ê±° ì¡°ê±´ ê²€ì¦
            if operator == ">=":
                triggered = severity >= threshold
            elif operator == "<=":
                triggered = severity <= threshold
            else:
                triggered = False
            
            assert triggered == expected, f"íŠ¸ë¦¬ê±° ì¡°ê±´ ì‹¤íŒ¨: {case}"


class TestMonteCarloPricing:
    """Monte Carlo ê°€ê²© ê³„ì‚° í…ŒìŠ¤íŠ¸"""
    
    def test_pricing_metrics_calculation(self):
        """ê°€ê²© ì§€í‘œ ê³„ì‚° ê²€ì¦"""
        
        # Mock ì—°ê°„ ì†ì‹¤ ë°ì´í„°
        annual_losses = [0, 5000, 12000, 0, 8500, 25000, 0, 3000, 15000, 45000]
        
        # Expected Loss ê³„ì‚°
        expected_loss = sum(annual_losses) / len(annual_losses)
        assert expected_loss == 11350.0
        
        # Coefficient of Variation ê³„ì‚°
        mean_loss = expected_loss
        variance = sum((x - mean_loss) ** 2 for x in annual_losses) / len(annual_losses)
        std_loss = variance ** 0.5
        cov = std_loss / mean_loss if mean_loss > 0 else 0
        
        assert cov > 0, "ë³€ë™ê³„ìˆ˜ëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨"
        
        # Risk Load ê³„ì‚° (0.15 + 0.5 Ã— CoV)
        risk_load = 0.15 + (0.5 * cov)
        assert risk_load >= 0.15, "Risk LoadëŠ” ìµœì†Œ 15%"
        
        # Gross Premium ê³„ì‚°
        gross_premium = expected_loss * (1 + risk_load)
        assert gross_premium > expected_loss, "ì´ ë³´í—˜ë£ŒëŠ” ê¸°ëŒ“ê°’ ì†ì‹¤ë³´ë‹¤ ì»¤ì•¼ í•¨"
    
    def test_var_tvar_calculation(self):
        """VaR/TVaR ê³„ì‚° ê²€ì¦"""
        
        # ì •ë ¬ëœ ì†ì‹¤ ë°ì´í„° (100ê°œ ì‹œë‚˜ë¦¬ì˜¤ ê°€ì •)
        sorted_losses = list(range(0, 100000, 1000))  # 0, 1000, 2000, ..., 99000
        
        # 99% VaR (99ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜)
        var_index = int(0.99 * len(sorted_losses)) - 1
        var_99 = sorted_losses[var_index]
        
        # 99% TVaR (VaR ì´ˆê³¼ ì†ì‹¤ì˜ í‰ê· )
        tail_losses = sorted_losses[var_index:]
        tvar_99 = sum(tail_losses) / len(tail_losses) if tail_losses else var_99
        
        assert tvar_99 >= var_99, "TVaRëŠ” VaRë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•¨"
        assert var_99 > 0, "VaRëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨"
    
    def test_risk_level_classification(self):
        """ë¦¬ìŠ¤í¬ ë ˆë²¨ ë¶„ë¥˜ ê²€ì¦"""
        
        test_cases = [
            {"cov": 0.2, "pml_ratio": 3, "expected": "low"},
            {"cov": 0.4, "pml_ratio": 7, "expected": "medium"},
            {"cov": 0.8, "pml_ratio": 15, "expected": "high"},
            {"cov": 1.5, "pml_ratio": 25, "expected": "very_high"}
        ]
        
        for case in test_cases:
            cov = case["cov"]
            pml_ratio = case["pml_ratio"]
            expected = case["expected"]
            
            # ë¦¬ìŠ¤í¬ ë ˆë²¨ ë¶„ë¥˜ ë¡œì§
            if cov < 0.3 and pml_ratio < 5:
                risk_level = "low"
            elif cov < 0.6 and pml_ratio < 10:
                risk_level = "medium"
            elif cov < 1.0 and pml_ratio < 20:
                risk_level = "high"
            else:
                risk_level = "very_high"
            
            assert risk_level == expected, f"ë¦¬ìŠ¤í¬ ë ˆë²¨ ë¶„ë¥˜ ì˜¤ë¥˜: {case}"


class TestValidationChecks:
    """ê²€ì¦ ì²´í¬ í…ŒìŠ¤íŠ¸"""
    
    def test_tail_padding_validation(self):
        """Tail Padding ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        
        test_cases = [
            {"risk_load": 0.25, "expected": True},  # 25% > 20%
            {"risk_load": 0.15, "expected": False}, # 15% < 20%
            {"risk_load": 0.20, "expected": True}   # 20% = 20%
        ]
        
        for case in test_cases:
            risk_load = case["risk_load"]
            expected = case["expected"]
            
            # Tail Padding ê²€ì¦ ë¡œì§
            min_risk_load = 0.20
            tail_padding_passed = risk_load >= min_risk_load
            
            assert tail_padding_passed == expected, f"Tail Padding ê²€ì¦ ì‹¤íŒ¨: {case}"
    
    def test_sanity_checks(self):
        """ê±´ì „ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        
        # Mock ê°€ê²©ì±…ì • ê²°ê³¼
        mock_result = {
            "expected_loss": 50000,
            "net_premium": 50000,
            "gross_premium": 65000,
            "var_99": 200000,
            "tvar_99": 250000,
            "coefficient_of_variation": 0.4,
            "risk_load": 0.3
        }
        
        # ê°ì¢… ê²€ì¦ ìˆ˜í–‰
        checks = {}
        
        # 1. ë³´í—˜ë£Œ ì¼ê´€ì„±
        checks["premium_consistency"] = mock_result["gross_premium"] >= mock_result["net_premium"]
        
        # 2. VaR/TVaR ê´€ê³„
        checks["var_tvar_consistency"] = mock_result["tvar_99"] >= mock_result["var_99"]
        
        # 3. ê¸°ëŒ“ê°’ ì†ì‹¤ ì–‘ìˆ˜
        checks["positive_el"] = mock_result["expected_loss"] >= 0
        
        # 4. CoV í•©ë¦¬ì„±
        checks["cov_reasonable"] = mock_result["coefficient_of_variation"] < 5.0
        
        # 5. Risk Load ë²”ìœ„
        checks["risk_load_range"] = 0 <= mock_result["risk_load"] <= 2.0
        
        # ëª¨ë“  ê²€ì¦ì´ í†µê³¼í•´ì•¼ í•¨
        assert all(checks.values()), f"ê²€ì¦ ì‹¤íŒ¨: {checks}"


class TestEndToEndWorkflow:
    """End-to-End ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸"""
    
    def test_workflow_state_transitions(self):
        """ì›Œí¬í”Œë¡œ ìƒíƒœ ì „í™˜ í…ŒìŠ¤íŠ¸"""
        
        # ì´ˆê¸° ìƒíƒœ
        initial_state = {
            "messages": [{"role": "user", "content": "ê²Œì„ ì„œë²„ ë‹¤ìš´íƒ€ì„ ë³´í—˜"}],
            "plan": "",
            "result": None,
            "event_type": None
        }
        
        # 1ë‹¨ê³„: Peril Canvas ìƒì„± í›„ ìƒíƒœ
        after_canvas = {
            **initial_state,
            "peril_canvas": MOCK_PERIL_CANVAS,
            "event_type": "server_downtime"
        }
        
        # 2ë‹¨ê³„: Prior ì¶”ì¶œ í›„ ìƒíƒœ
        after_prior = {
            **after_canvas,
            "frequency_prior": MOCK_FREQUENCY_PRIOR,
            "severity_prior": MOCK_SEVERITY_PRIOR
        }
        
        # 3ë‹¨ê³„: ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± í›„ ìƒíƒœ
        after_scenarios = {
            **after_prior,
            "scenarios": {
                "summary": {"total_scenarios": 100, "mean_annual_loss": 25000},
                "data": []  # ì‹¤ì œë¡œëŠ” ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
            }
        }
        
        # ê° ë‹¨ê³„ë³„ í•„ìˆ˜ ë°ì´í„° í™•ì¸
        assert "peril_canvas" in after_canvas
        assert "event_type" in after_canvas
        
        assert "frequency_prior" in after_prior
        assert "severity_prior" in after_prior
        
        assert "scenarios" in after_scenarios
        assert "summary" in after_scenarios["scenarios"]
    
    def test_error_handling(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        # ë°ì´í„° ëˆ„ë½ ì‹œë‚˜ë¦¬ì˜¤
        incomplete_states = [
            {},  # ë¹ˆ ìƒíƒœ
            {"messages": []},  # ë©”ì‹œì§€ ì—†ìŒ
            {"peril_canvas": None},  # Canvas ì—†ìŒ
            {"frequency_prior": None, "severity_prior": None}  # Prior ì—†ìŒ
        ]
        
        for state in incomplete_states:
            # ê° ìƒíƒœì—ì„œ í•„ìˆ˜ ë°ì´í„° í™•ì¸
            has_messages = bool(state.get("messages"))
            has_canvas = bool(state.get("peril_canvas"))
            has_priors = bool(state.get("frequency_prior")) and bool(state.get("severity_prior"))
            
            # ìµœì†Œí•œì˜ ìš”êµ¬ì‚¬í•­ í™•ì¸
            if not has_messages:
                assert "messages" not in state or not state["messages"]
            
            if not has_canvas:
                assert "peril_canvas" not in state or not state["peril_canvas"]
            
            if not has_priors:
                assert not (state.get("frequency_prior") and state.get("severity_prior"))


# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    test_classes = [
        TestPerilCanvasGeneration(),
        TestPriorExtraction(),
        TestScenarioGeneration(),
        TestMonteCarloPricing(),
        TestValidationChecks(),
        TestEndToEndWorkflow()
    ]
    
    total_tests = 0
    passed_tests = 0
    failed_tests = []
    
    for test_class in test_classes:
        class_name = test_class.__class__.__name__
        print(f"\nğŸ§ª {class_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        
        # ê° í´ë˜ìŠ¤ì˜ í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰
        for method_name in dir(test_class):
            if method_name.startswith("test_"):
                total_tests += 1
                try:
                    method = getattr(test_class, method_name)
                    method()
                    print(f"   âœ… {method_name}")
                    passed_tests += 1
                except Exception as e:
                    print(f"   âŒ {method_name}: {str(e)}")
                    failed_tests.append((class_name, method_name, str(e)))
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
    print(f"   í†µê³¼: {passed_tests}")
    print(f"   ì‹¤íŒ¨: {len(failed_tests)}")
    print(f"   ì„±ê³µë¥ : {passed_tests/total_tests*100:.1f}%")
    
    if failed_tests:
        print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
        for class_name, method_name, error in failed_tests:
            print(f"   {class_name}.{method_name}: {error}")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    print("ğŸš€ LLM-Lite Parametric Pricing í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    success = run_all_tests()
    
    if success:
        print(f"\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
        print("   ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   ì‹¤íŒ¨í•œ ë¶€ë¶„ì„ ê²€í† í•´ì£¼ì„¸ìš”.")
    
    print("\n" + "=" * 60)